{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1M8JlP6kfZ8",
        "outputId": "43513064-83a9-402e-ee83-8173a2d1ac4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2bqzeJHkKpO",
        "outputId": "e22d5012-cd88-4247-ccbc-c09a5e8b7770"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "\n",
            "Epoch 050, Loss: 0.4058\n",
            "\n",
            "Train Metrics:\n",
            "Accuracy: 0.8355\n",
            "Class 0 Sensitivity: 0.8430, Specificity: 0.9516\n",
            "Class 1 Sensitivity: 0.8239, Specificity: 0.8455\n",
            "Class 2 Sensitivity: 0.8466, Specificity: 0.9416\n",
            "AUC (macro): 0.9517\n",
            "AUC (weighted): 0.9457\n",
            "Class 0 AUC: 0.9722\n",
            "Class 1 AUC: 0.9119\n",
            "Class 2 AUC: 0.9711\n",
            "\n",
            "Epoch 100, Loss: 0.3701\n",
            "\n",
            "Train Metrics:\n",
            "Accuracy: 0.8480\n",
            "Class 0 Sensitivity: 0.9083, Specificity: 0.9356\n",
            "Class 1 Sensitivity: 0.7866, Specificity: 0.8970\n",
            "Class 2 Sensitivity: 0.8748, Specificity: 0.9339\n",
            "AUC (macro): 0.9587\n",
            "AUC (weighted): 0.9539\n",
            "Class 0 AUC: 0.9771\n",
            "Class 1 AUC: 0.9258\n",
            "Class 2 AUC: 0.9732\n",
            "\n",
            "Epoch 150, Loss: 0.3659\n",
            "\n",
            "Train Metrics:\n",
            "Accuracy: 0.8513\n",
            "Class 0 Sensitivity: 0.9019, Specificity: 0.9418\n",
            "Class 1 Sensitivity: 0.8016, Specificity: 0.8912\n",
            "Class 2 Sensitivity: 0.8700, Specificity: 0.9370\n",
            "AUC (macro): 0.9599\n",
            "AUC (weighted): 0.9553\n",
            "Class 0 AUC: 0.9781\n",
            "Class 1 AUC: 0.9279\n",
            "Class 2 AUC: 0.9738\n",
            "\n",
            "Epoch 200, Loss: 0.3656\n",
            "\n",
            "Train Metrics:\n",
            "Accuracy: 0.8505\n",
            "Class 0 Sensitivity: 0.9011, Specificity: 0.9420\n",
            "Class 1 Sensitivity: 0.8031, Specificity: 0.8887\n",
            "Class 2 Sensitivity: 0.8646, Specificity: 0.9377\n",
            "AUC (macro): 0.9601\n",
            "AUC (weighted): 0.9555\n",
            "Class 0 AUC: 0.9781\n",
            "Class 1 AUC: 0.9283\n",
            "Class 2 AUC: 0.9739\n",
            "\n",
            "Epoch 250, Loss: 0.3678\n",
            "\n",
            "Train Metrics:\n",
            "Accuracy: 0.8505\n",
            "Class 0 Sensitivity: 0.9011, Specificity: 0.9418\n",
            "Class 1 Sensitivity: 0.8028, Specificity: 0.8889\n",
            "Class 2 Sensitivity: 0.8652, Specificity: 0.9377\n",
            "AUC (macro): 0.9601\n",
            "AUC (weighted): 0.9556\n",
            "Class 0 AUC: 0.9781\n",
            "Class 1 AUC: 0.9283\n",
            "Class 2 AUC: 0.9739\n",
            "\n",
            "Epoch 300, Loss: 0.3629\n",
            "\n",
            "Train Metrics:\n",
            "Accuracy: 0.8509\n",
            "Class 0 Sensitivity: 0.9011, Specificity: 0.9418\n",
            "Class 1 Sensitivity: 0.8028, Specificity: 0.8896\n",
            "Class 2 Sensitivity: 0.8670, Specificity: 0.9377\n",
            "AUC (macro): 0.9601\n",
            "AUC (weighted): 0.9555\n",
            "Class 0 AUC: 0.9781\n",
            "Class 1 AUC: 0.9283\n",
            "Class 2 AUC: 0.9739\n",
            "\n",
            "Epoch 350, Loss: 0.3672\n",
            "\n",
            "Train Metrics:\n",
            "Accuracy: 0.8509\n",
            "Class 0 Sensitivity: 0.9011, Specificity: 0.9418\n",
            "Class 1 Sensitivity: 0.8028, Specificity: 0.8896\n",
            "Class 2 Sensitivity: 0.8670, Specificity: 0.9377\n",
            "AUC (macro): 0.9601\n",
            "AUC (weighted): 0.9555\n",
            "Class 0 AUC: 0.9781\n",
            "Class 1 AUC: 0.9283\n",
            "Class 2 AUC: 0.9739\n",
            "\n",
            "Epoch 400, Loss: 0.3612\n",
            "\n",
            "Train Metrics:\n",
            "Accuracy: 0.8505\n",
            "Class 0 Sensitivity: 0.9015, Specificity: 0.9416\n",
            "Class 1 Sensitivity: 0.8025, Specificity: 0.8891\n",
            "Class 2 Sensitivity: 0.8652, Specificity: 0.9377\n",
            "AUC (macro): 0.9601\n",
            "AUC (weighted): 0.9555\n",
            "Class 0 AUC: 0.9782\n",
            "Class 1 AUC: 0.9283\n",
            "Class 2 AUC: 0.9739\n",
            "\n",
            "Epoch 450, Loss: 0.3682\n",
            "\n",
            "Train Metrics:\n",
            "Accuracy: 0.8508\n",
            "Class 0 Sensitivity: 0.9011, Specificity: 0.9418\n",
            "Class 1 Sensitivity: 0.8028, Specificity: 0.8894\n",
            "Class 2 Sensitivity: 0.8664, Specificity: 0.9377\n",
            "AUC (macro): 0.9601\n",
            "AUC (weighted): 0.9555\n",
            "Class 0 AUC: 0.9781\n",
            "Class 1 AUC: 0.9283\n",
            "Class 2 AUC: 0.9740\n",
            "\n",
            "Epoch 500, Loss: 0.3590\n",
            "\n",
            "Train Metrics:\n",
            "Accuracy: 0.8511\n",
            "Class 0 Sensitivity: 0.9011, Specificity: 0.9420\n",
            "Class 1 Sensitivity: 0.8031, Specificity: 0.8896\n",
            "Class 2 Sensitivity: 0.8670, Specificity: 0.9377\n",
            "AUC (macro): 0.9601\n",
            "AUC (weighted): 0.9555\n",
            "Class 0 AUC: 0.9781\n",
            "Class 1 AUC: 0.9283\n",
            "Class 2 AUC: 0.9739\n",
            "\n",
            "Loading best model from epoch 150...\n",
            "\n",
            "Evaluating on test set...\n",
            "\n",
            "Test Metrics:\n",
            "Accuracy: 0.8585\n",
            "Class 0 Sensitivity: 0.9073, Specificity: 0.9500\n",
            "Class 1 Sensitivity: 0.7872, Specificity: 0.9034\n",
            "Class 2 Sensitivity: 0.8942, Specificity: 0.9335\n",
            "AUC (macro): 0.9560\n",
            "AUC (weighted): 0.9529\n",
            "Class 0 AUC: 0.9738\n",
            "Class 1 AUC: 0.9172\n",
            "Class 2 AUC: 0.9770\n",
            "\n",
            "Test Confusion Matrix:\n",
            "[[656  66   1]\n",
            " [ 56 555  94]\n",
            " [  0  44 372]]\n",
            "\n",
            "Test Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9213    0.9073    0.9143       723\n",
            "           1     0.8346    0.7872    0.8102       705\n",
            "           2     0.7966    0.8942    0.8426       416\n",
            "\n",
            "    accuracy                         0.8585      1844\n",
            "   macro avg     0.8508    0.8629    0.8557      1844\n",
            "weighted avg     0.8600    0.8585    0.8583      1844\n",
            "\n",
            "\n",
            "Metrics saved to model_metrics.csv\n",
            "\n",
            "===== SUMMARY =====\n",
            "Best epoch: 150\n",
            "Train accuracy: 0.8513\n",
            "Test accuracy: 0.8585\n",
            "Train AUC (macro): 0.9599\n",
            "Test AUC (macro): 0.9560\n",
            "\n",
            "===== GENERATING PREDICTION CSVs =====\n",
            "Using 1 models for ensemble predictions\n",
            "Created test visit-level predictions: test_visit_predictions.csv\n",
            "Created test patient-level predictions: test_patient_predictions.csv\n",
            "Created train visit-level predictions: train_visit_predictions.csv\n",
            "Created train patient-level predictions: train_patient_predictions.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATConv\n",
        "from sklearn.metrics import (confusion_matrix, classification_report,\n",
        "                            roc_auc_score, roc_curve, auc, accuracy_score)\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import re\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import glob\n",
        "######################################\n",
        "# Evaluation Metrics Functions\n",
        "######################################\n",
        "def calculate_metrics(y_true, y_pred, y_prob=None):\n",
        "    \"\"\"Calculate accuracy, sensitivity, specificity for each class and overall\"\"\"\n",
        "    metrics = {}\n",
        "\n",
        "    # Overall accuracy\n",
        "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    # Per-class metrics\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # For each class\n",
        "    n_classes = len(np.unique(y_true))\n",
        "    for i in range(n_classes):\n",
        "        # True positives, false positives, true negatives, false negatives\n",
        "        tp = cm[i, i]\n",
        "        fp = np.sum(cm[:, i]) - tp\n",
        "        fn = np.sum(cm[i, :]) - tp\n",
        "        tn = np.sum(cm) - tp - fp - fn\n",
        "\n",
        "        # Sensitivity (recall) = TP / (TP + FN)\n",
        "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        metrics[f'sensitivity_class_{i}'] = sensitivity\n",
        "\n",
        "        # Specificity = TN / (TN + FP)\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "        metrics[f'specificity_class_{i}'] = specificity\n",
        "\n",
        "    # Calculate AUC-ROC if probabilities are provided\n",
        "    if y_prob is not None:\n",
        "        if n_classes > 2:\n",
        "            # For multiclass, convert to one-hot encoding for ROC AUC calculation\n",
        "            y_true_bin = label_binarize(y_true, classes=range(n_classes))\n",
        "\n",
        "            # Calculate AUC for each class\n",
        "            for i in range(n_classes):\n",
        "                if len(np.unique(y_true_bin[:, i])) > 1:  # Only calculate if both classes are present\n",
        "                    metrics[f'auc_class_{i}'] = roc_auc_score(y_true_bin[:, i], y_prob[:, i])\n",
        "\n",
        "            # Calculate macro and weighted AUC\n",
        "            metrics['auc_macro'] = roc_auc_score(y_true_bin, y_prob, average='macro', multi_class='ovr')\n",
        "            metrics['auc_weighted'] = roc_auc_score(y_true_bin, y_prob, average='weighted', multi_class='ovr')\n",
        "        else:\n",
        "            # Binary case\n",
        "            metrics['auc'] = roc_auc_score(y_true, y_prob[:, 1])\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def plot_roc_curves(y_true, y_prob, n_classes, phase='train'):\n",
        "    \"\"\"Plot ROC curves for each class\"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # One-hot encode the labels for ROC calculation\n",
        "    y_true_bin = label_binarize(y_true, classes=range(n_classes))\n",
        "\n",
        "    # Plot ROC curve for each class\n",
        "    for i in range(n_classes):\n",
        "        fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, lw=2, label=f'Class {i} (AUC = {roc_auc:.4f})')\n",
        "\n",
        "    # Plot diagonal line\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'ROC Curves for {phase.capitalize()} Set')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.savefig(f'roc_curve_{phase}.png')\n",
        "    plt.close()\n",
        "\n",
        "######################################\n",
        "# Connectivity Building Function (Time-based)\n",
        "######################################\n",
        "def build_time_based_connectivity(df, time_threshold=18, tau=10):\n",
        "    visit_edge_index = [[], []]\n",
        "    visit_edge_weight_list = []\n",
        "    patient_to_visit_indices = {}\n",
        "    for i, ptid in enumerate(df['PTID']):\n",
        "        patient_to_visit_indices.setdefault(ptid, []).append(i)\n",
        "\n",
        "    for indices in patient_to_visit_indices.values():\n",
        "        sorted_indices = sorted(indices, key=lambda idx: df.iloc[idx]['time'])\n",
        "        n = len(sorted_indices)\n",
        "        for i in range(n):\n",
        "            for j in range(i+1, n):\n",
        "                time_i = df.iloc[sorted_indices[i]]['time']\n",
        "                time_j = df.iloc[sorted_indices[j]]['time']\n",
        "                gap = abs(time_j - time_i)\n",
        "                if gap <= time_threshold:\n",
        "                    weight = np.exp(-gap / tau)\n",
        "                    # Bidirectional edges\n",
        "                    visit_edge_index[0].append(sorted_indices[i])\n",
        "                    visit_edge_index[1].append(sorted_indices[j])\n",
        "                    visit_edge_weight_list.append(weight)\n",
        "                    visit_edge_index[0].append(sorted_indices[j])\n",
        "                    visit_edge_index[1].append(sorted_indices[i])\n",
        "                    visit_edge_weight_list.append(weight)\n",
        "    visit_edge_index = torch.tensor(visit_edge_index, dtype=torch.long)\n",
        "    visit_edge_weight = torch.tensor(visit_edge_weight_list, dtype=torch.float)\n",
        "    return visit_edge_index, visit_edge_weight\n",
        "\n",
        "######################################\n",
        "# Helper function for viscode conversion\n",
        "######################################\n",
        "def convert_viscode(viscode):\n",
        "    if isinstance(viscode, str):\n",
        "        if viscode.lower() == \"sc\":\n",
        "            return 0\n",
        "        m = re.match(r'm(\\d+)', viscode.lower())\n",
        "        if m:\n",
        "            return int(m.group(1))\n",
        "    return None\n",
        "\n",
        "######################################\n",
        "# Load and preprocess training data\n",
        "######################################\n",
        "train_file = \"/content/train_data.csv\"\n",
        "train_df = pd.read_csv(train_file)\n",
        "train_df = train_df[train_df['DIAGNOSIS'].isin([1, 2, 3])]\n",
        "train_df['DIAGNOSIS'] = train_df['DIAGNOSIS'] - 1\n",
        "train_df = train_df.fillna(0)\n",
        "\n",
        "# Define feature columns - exclude genotype\n",
        "non_feature_cols = ['PTID', 'VISCODE2', 'DIAGNOSIS', 'GENOTYPE']  # Added GENOTYPE to non-features\n",
        "feature_columns = [col for col in train_df.columns if col not in non_feature_cols]\n",
        "\n",
        "# Remove any genotype dummy columns if they exist\n",
        "genotype_cols = [col for col in train_df.columns if col.startswith('GENOTYPE_')]\n",
        "for col in genotype_cols:\n",
        "    if col in feature_columns:\n",
        "        feature_columns.remove(col)\n",
        "\n",
        "# Calculate mean and std for standardization\n",
        "feature_means = train_df[feature_columns].mean()\n",
        "feature_stds = train_df[feature_columns].std()\n",
        "train_df[feature_columns] = (train_df[feature_columns] - feature_means) / feature_stds\n",
        "\n",
        "# Process time\n",
        "train_df['time'] = train_df['VISCODE2'].apply(convert_viscode)\n",
        "train_df = train_df[train_df['time'].notnull()]\n",
        "\n",
        "# Extract labels and features\n",
        "train_labels = train_df['DIAGNOSIS'].values\n",
        "train_features = train_df[feature_columns].values.astype(np.float32)\n",
        "\n",
        "# Compute class weights\n",
        "classes = np.unique(train_labels)\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_labels)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "\n",
        "# Build visit-level connectivity\n",
        "train_visit_edge_index, train_visit_edge_weight = build_time_based_connectivity(train_df, time_threshold=18, tau=10)\n",
        "train_visit_x = torch.tensor(train_features, dtype=torch.float)\n",
        "train_y_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
        "\n",
        "# Create train graph data\n",
        "train_visit_data = Data(x=train_visit_x, edge_index=train_visit_edge_index, y=train_y_tensor)\n",
        "train_visit_data.edge_weight = train_visit_edge_weight\n",
        "\n",
        "# Build patient-level graph for training\n",
        "train_patient_df = train_df.groupby('PTID')[feature_columns].mean().reset_index()\n",
        "train_patient_features = train_patient_df[feature_columns].values.astype(np.float32)\n",
        "train_patient_x = torch.tensor(train_patient_features, dtype=torch.float)\n",
        "\n",
        "n_neighbors = min(4, len(train_patient_features) - 1)  # Ensure we don't exceed available samples\n",
        "nbrs = NearestNeighbors(n_neighbors=n_neighbors)\n",
        "nbrs.fit(train_patient_features)\n",
        "_, knn_indices = nbrs.kneighbors(train_patient_features)\n",
        "\n",
        "train_patient_edge_index = [[], []]\n",
        "num_patients = train_patient_features.shape[0]\n",
        "for i in range(num_patients):\n",
        "    for j in knn_indices[i][1:]:  # Skip the first neighbor (self)\n",
        "        train_patient_edge_index[0].append(i)\n",
        "        train_patient_edge_index[1].append(j)\n",
        "        train_patient_edge_index[0].append(j)\n",
        "        train_patient_edge_index[1].append(i)\n",
        "train_patient_edge_index = torch.tensor(train_patient_edge_index, dtype=torch.long)\n",
        "\n",
        "train_ptid_to_patient_index = {ptid: idx for idx, ptid in enumerate(train_patient_df['PTID'])}\n",
        "train_visit_to_patient_mapping = train_df['PTID'].apply(lambda ptid: train_ptid_to_patient_index[ptid]).values\n",
        "train_visit_to_patient_mapping = torch.tensor(train_visit_to_patient_mapping, dtype=torch.long)\n",
        "\n",
        "# Create a mask for all training data\n",
        "train_mask = torch.ones(len(train_labels), dtype=torch.bool)\n",
        "train_visit_data.train_mask = train_mask\n",
        "\n",
        "######################################\n",
        "# Load and preprocess test data\n",
        "######################################\n",
        "test_file = \"/content/test_data.csv\"\n",
        "test_df = pd.read_csv(test_file)\n",
        "test_df = test_df[test_df['DIAGNOSIS'].isin([1, 2, 3])]\n",
        "test_df['DIAGNOSIS'] = test_df['DIAGNOSIS'] - 1\n",
        "test_df = test_df.fillna(0)\n",
        "\n",
        "# Use the same feature columns as training (already excludes GENOTYPE)\n",
        "test_feature_columns = [col for col in feature_columns if col in test_df.columns]\n",
        "\n",
        "# Standardize using training means and stds\n",
        "for col in test_feature_columns:\n",
        "    if col in feature_means and col in feature_stds:\n",
        "        test_df[col] = (test_df[col] - feature_means[col]) / feature_stds[col]\n",
        "\n",
        "# Process time\n",
        "test_df['time'] = test_df['VISCODE2'].apply(convert_viscode)\n",
        "test_df = test_df[test_df['time'].notnull()]\n",
        "\n",
        "# Extract labels and features\n",
        "test_labels = test_df['DIAGNOSIS'].values\n",
        "test_features = test_df[test_feature_columns].values.astype(np.float32)\n",
        "\n",
        "# Build visit-level connectivity\n",
        "test_visit_edge_index, test_visit_edge_weight = build_time_based_connectivity(test_df, time_threshold=18, tau=10)\n",
        "test_visit_x = torch.tensor(test_features, dtype=torch.float)\n",
        "test_y_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
        "\n",
        "# Create test graph data\n",
        "test_visit_data = Data(x=test_visit_x, edge_index=test_visit_edge_index, y=test_y_tensor)\n",
        "test_visit_data.edge_weight = test_visit_edge_weight\n",
        "\n",
        "# Build patient-level graph for testing\n",
        "test_patient_df = test_df.groupby('PTID')[test_feature_columns].mean().reset_index()\n",
        "test_patient_features = test_patient_df[test_feature_columns].values.astype(np.float32)\n",
        "test_patient_x = torch.tensor(test_patient_features, dtype=torch.float)\n",
        "\n",
        "n_neighbors = min(4, len(test_patient_features) - 1)  # Ensure we don't exceed available samples\n",
        "nbrs = NearestNeighbors(n_neighbors=n_neighbors)\n",
        "nbrs.fit(test_patient_features)\n",
        "_, knn_indices = nbrs.kneighbors(test_patient_features)\n",
        "\n",
        "test_patient_edge_index = [[], []]\n",
        "num_patients = test_patient_features.shape[0]\n",
        "for i in range(num_patients):\n",
        "    for j in knn_indices[i][1:]:  # Skip the first neighbor (self)\n",
        "        test_patient_edge_index[0].append(i)\n",
        "        test_patient_edge_index[1].append(j)\n",
        "        test_patient_edge_index[0].append(j)\n",
        "        test_patient_edge_index[1].append(i)\n",
        "test_patient_edge_index = torch.tensor(test_patient_edge_index, dtype=torch.long)\n",
        "\n",
        "test_ptid_to_patient_index = {ptid: idx for idx, ptid in enumerate(test_patient_df['PTID'])}\n",
        "test_visit_to_patient_mapping = test_df['PTID'].apply(lambda ptid: test_ptid_to_patient_index[ptid]).values\n",
        "test_visit_to_patient_mapping = torch.tensor(test_visit_to_patient_mapping, dtype=torch.long)\n",
        "\n",
        "# Create a mask for all test data\n",
        "test_mask = torch.ones(len(test_labels), dtype=torch.bool)\n",
        "test_visit_data.test_mask = test_mask\n",
        "\n",
        "######################################\n",
        "# Define the Multi-Scale GAT Model\n",
        "######################################\n",
        "class MultiScaleGAT(nn.Module):\n",
        "    def __init__(self, visit_in_channels, patient_in_channels, hidden_channels, num_classes, dropout=0.3, num_heads=4):\n",
        "        super(MultiScaleGAT, self).__init__()\n",
        "        # Visit-level branch (4 layers)\n",
        "        self.visit_gat1 = GATConv(visit_in_channels, hidden_channels, heads=num_heads, concat=False, dropout=dropout)\n",
        "        self.visit_bn1 = nn.BatchNorm1d(hidden_channels)\n",
        "        self.visit_gat2 = GATConv(hidden_channels, hidden_channels, heads=num_heads, concat=False, dropout=dropout)\n",
        "        self.visit_bn2 = nn.BatchNorm1d(hidden_channels)\n",
        "        self.visit_gat3 = GATConv(hidden_channels, hidden_channels, heads=num_heads, concat=False, dropout=dropout)\n",
        "        self.visit_bn3 = nn.BatchNorm1d(hidden_channels)\n",
        "        self.visit_gat4 = GATConv(hidden_channels, hidden_channels, heads=num_heads, concat=False, dropout=dropout)\n",
        "        self.visit_bn4 = nn.BatchNorm1d(hidden_channels)\n",
        "        self.visit_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Patient-level branch (3 layers)\n",
        "        self.patient_gat1 = GATConv(patient_in_channels, hidden_channels, heads=num_heads, concat=False, dropout=dropout)\n",
        "        self.patient_bn1 = nn.BatchNorm1d(hidden_channels)\n",
        "        self.patient_gat2 = GATConv(hidden_channels, hidden_channels, heads=num_heads, concat=False, dropout=dropout)\n",
        "        self.patient_bn2 = nn.BatchNorm1d(hidden_channels)\n",
        "        self.patient_gat3 = GATConv(hidden_channels, hidden_channels, heads=num_heads, concat=False, dropout=dropout)\n",
        "        self.patient_bn3 = nn.BatchNorm1d(hidden_channels)\n",
        "        self.patient_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_channels * 2, num_classes)\n",
        "\n",
        "    def forward(self, visit_x, visit_edge_index, patient_x, patient_edge_index, visit_to_patient_mapping):\n",
        "        # Visit-level branch\n",
        "        v = self.visit_gat1(visit_x, visit_edge_index)\n",
        "        v = self.visit_bn1(v)\n",
        "        v = F.relu(v)\n",
        "        v = self.visit_dropout(v)\n",
        "        v_res = v\n",
        "        v = self.visit_gat2(v, visit_edge_index)\n",
        "        v = self.visit_bn2(v)\n",
        "        v = F.relu(v)\n",
        "        v = v + v_res\n",
        "        v = self.visit_dropout(v)\n",
        "        v_res = v\n",
        "        v = self.visit_gat3(v, visit_edge_index)\n",
        "        v = self.visit_bn3(v)\n",
        "        v = F.relu(v)\n",
        "        v = v + v_res\n",
        "        v = self.visit_dropout(v)\n",
        "        v_res = v\n",
        "        v = self.visit_gat4(v, visit_edge_index)\n",
        "        v = self.visit_bn4(v)\n",
        "        v = F.relu(v)\n",
        "        v = v + v_res\n",
        "        v = self.visit_dropout(v)\n",
        "\n",
        "        # Patient-level branch\n",
        "        p = self.patient_gat1(patient_x, patient_edge_index)\n",
        "        p = self.patient_bn1(p)\n",
        "        p = F.relu(p)\n",
        "        p = self.patient_dropout(p)\n",
        "        p_res = p\n",
        "        p = self.patient_gat2(p, patient_edge_index)\n",
        "        p = self.patient_bn2(p)\n",
        "        p = F.relu(p)\n",
        "        p = p + p_res\n",
        "        p = self.patient_dropout(p)\n",
        "        p_res = p\n",
        "        p = self.patient_gat3(p, patient_edge_index)\n",
        "        p = self.patient_bn3(p)\n",
        "        p = F.relu(p)\n",
        "        p = p + p_res\n",
        "        p = self.patient_dropout(p)\n",
        "\n",
        "        # Map patient embedding to each visit\n",
        "        p_for_visit = p[visit_to_patient_mapping]\n",
        "        combined = torch.cat([v, p_for_visit], dim=1)\n",
        "        logits = self.classifier(combined)\n",
        "        return logits, F.log_softmax(logits, dim=1)\n",
        "\n",
        "######################################\n",
        "# Training Setup and Training Loop\n",
        "######################################\n",
        "# Best hyperparameters from grid search\n",
        "num_classes = len(torch.unique(train_y_tensor))\n",
        "model = MultiScaleGAT(train_visit_x.shape[1], train_patient_x.shape[1],\n",
        "                      hidden_channels=128, num_classes=num_classes,\n",
        "                      dropout=0.3, num_heads=4)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0005)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
        "\n",
        "num_epochs = 500\n",
        "\n",
        "def compute_metrics_for_batch(model, data, patient_x, patient_edge_index, visit_to_patient_mapping, mask=None):\n",
        "    \"\"\"Compute all metrics for a batch of data\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Forward pass\n",
        "        logits, log_softmax = model(data.x, data.edge_index, patient_x, patient_edge_index, visit_to_patient_mapping)\n",
        "        preds = log_softmax.argmax(dim=1)\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is None:\n",
        "            mask = torch.ones(len(data.y), dtype=torch.bool)\n",
        "\n",
        "        # Get predictions, ground truth, and probabilities for masked entries\n",
        "        y_true = data.y[mask].cpu().numpy()\n",
        "        y_pred = preds[mask].cpu().numpy()\n",
        "        y_prob = F.softmax(logits[mask], dim=1).cpu().numpy()\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = calculate_metrics(y_true, y_pred, y_prob)\n",
        "        return metrics, y_true, y_pred, y_prob\n",
        "\n",
        "def print_metrics(metrics, phase='train'):\n",
        "    \"\"\"Print metrics in a formatted way\"\"\"\n",
        "    print(f\"\\n{phase.capitalize()} Metrics:\")\n",
        "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "\n",
        "    # Print per-class sensitivity and specificity\n",
        "    for key in sorted([k for k in metrics.keys() if 'sensitivity_class' in k]):\n",
        "        class_idx = key.split('_')[-1]\n",
        "        print(f\"Class {class_idx} Sensitivity: {metrics[key]:.4f}, \"\n",
        "              f\"Specificity: {metrics[f'specificity_class_{class_idx}']:.4f}\")\n",
        "\n",
        "    # Print AUC metrics if available\n",
        "    if 'auc_macro' in metrics:\n",
        "        print(f\"AUC (macro): {metrics['auc_macro']:.4f}\")\n",
        "        print(f\"AUC (weighted): {metrics['auc_weighted']:.4f}\")\n",
        "\n",
        "    # Print per-class AUC if available\n",
        "    for key in sorted([k for k in metrics.keys() if 'auc_class' in k]):\n",
        "        class_idx = key.split('_')[-1]\n",
        "        print(f\"Class {class_idx} AUC: {metrics[key]:.4f}\")\n",
        "def load_models(model_paths, model_class, model_params):\n",
        "    \"\"\"Load multiple trained models from saved checkpoints\"\"\"\n",
        "    models = []\n",
        "    for path in model_paths:\n",
        "        model = model_class(**model_params)\n",
        "        model.load_state_dict(torch.load(path))\n",
        "        model.eval()\n",
        "        models.append(model)\n",
        "    return models\n",
        "\n",
        "def get_ensemble_predictions(models, visit_data, patient_x, patient_edge_index, visit_to_patient_mapping):\n",
        "    \"\"\"Get predictions from multiple models and combine them\"\"\"\n",
        "    all_visit_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for model in models:\n",
        "            logits, _ = model(visit_data.x, visit_data.edge_index, patient_x,\n",
        "                             patient_edge_index, visit_to_patient_mapping)\n",
        "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
        "            all_visit_probs.append(probs)\n",
        "\n",
        "    # Average probabilities from all models\n",
        "    ensemble_probs = np.mean(all_visit_probs, axis=0)\n",
        "    ensemble_preds = np.argmax(ensemble_probs, axis=1)\n",
        "\n",
        "    return ensemble_preds, ensemble_probs\n",
        "\n",
        "def create_visit_level_csv(visit_df, predictions, probabilities, output_file=\"visit_predictions.csv\"):\n",
        "    \"\"\"Create a CSV with visit-level predictions\"\"\"\n",
        "    result_df = visit_df[['PTID', 'VISCODE2', 'DIAGNOSIS']].copy()\n",
        "\n",
        "    # Add predictions and diagnosis labels (0-indexed in model, 1-indexed in data)\n",
        "    result_df['DIAGNOSIS'] = result_df['DIAGNOSIS'] + 1  # Convert back to 1-indexed\n",
        "    result_df['PREDICTED'] = predictions + 1  # Convert back to 1-indexed\n",
        "\n",
        "    # Add probabilities for each class\n",
        "    for i in range(probabilities.shape[1]):\n",
        "        result_df[f'PROB_CLASS_{i+1}'] = probabilities[:, i]\n",
        "\n",
        "    result_df.to_csv(output_file, index=False)\n",
        "    return result_df\n",
        "\n",
        "def create_patient_level_csv(visit_df, visit_predictions, visit_probabilities, output_file=\"patient_predictions.csv\"):\n",
        "    \"\"\"Aggregate visit predictions to patient level and create a CSV\"\"\"\n",
        "    # Group by patient to get last diagnosis for each patient\n",
        "    patient_df = visit_df.groupby('PTID')['DIAGNOSIS'].last().reset_index()\n",
        "\n",
        "    # Create a DataFrame with visit predictions\n",
        "    visit_pred_df = pd.DataFrame({\n",
        "        'PTID': visit_df['PTID'],\n",
        "        'VISCODE2': visit_df['VISCODE2'],\n",
        "        'PREDICTED': visit_predictions + 1,  # Convert back to 1-indexed\n",
        "    })\n",
        "\n",
        "    # For each probability class\n",
        "    num_classes = visit_probabilities.shape[1]\n",
        "    for i in range(num_classes):\n",
        "        visit_pred_df[f'PROB_CLASS_{i+1}'] = visit_probabilities[:, i]\n",
        "\n",
        "    # Group by patient: for each patient get most frequent prediction\n",
        "    patient_predictions = []\n",
        "    patient_probabilities = []\n",
        "\n",
        "    for ptid in patient_df['PTID']:\n",
        "        # Get all visits for this patient\n",
        "        patient_visits = visit_pred_df[visit_pred_df['PTID'] == ptid]\n",
        "\n",
        "        # Get the most frequent prediction (majority voting)\n",
        "        pred_counts = patient_visits['PREDICTED'].value_counts()\n",
        "        most_frequent_pred = pred_counts.index[0]\n",
        "\n",
        "        # Average the probabilities across all visits\n",
        "        avg_probs = []\n",
        "        for i in range(1, num_classes+1):\n",
        "            avg_probs.append(patient_visits[f'PROB_CLASS_{i}'].mean())\n",
        "\n",
        "        patient_predictions.append(most_frequent_pred)\n",
        "        patient_probabilities.append(avg_probs)\n",
        "\n",
        "    # Add predictions to patient dataframe\n",
        "    patient_df['PREDICTED'] = patient_predictions\n",
        "\n",
        "    # Add averaged probabilities\n",
        "    patient_probabilities = np.array(patient_probabilities)\n",
        "    for i in range(num_classes):\n",
        "        patient_df[f'PROB_CLASS_{i+1}'] = patient_probabilities[:, i]\n",
        "\n",
        "    patient_df.to_csv(output_file, index=False)\n",
        "    return patient_df\n",
        "\n",
        "# Training loop with evaluation metrics\n",
        "print(\"Starting training...\")\n",
        "best_train_metrics = None\n",
        "best_epoch = 0\n",
        "best_val_acc = 0\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    _, out = model(train_visit_data.x, train_visit_data.edge_index,\n",
        "                train_patient_x, train_patient_edge_index, train_visit_to_patient_mapping)\n",
        "    loss = F.nll_loss(out[train_visit_data.train_mask],\n",
        "                     train_visit_data.y[train_visit_data.train_mask],\n",
        "                     weight=class_weights)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step(loss.item())\n",
        "\n",
        "    # Evaluate metrics every 50 epochs or at the end\n",
        "    if epoch % 50 == 0 or epoch == num_epochs:\n",
        "        # Calculate and print training metrics\n",
        "        train_metrics, y_true_train, y_pred_train, y_prob_train = compute_metrics_for_batch(\n",
        "            model, train_visit_data, train_patient_x, train_patient_edge_index,\n",
        "            train_visit_to_patient_mapping, train_visit_data.train_mask\n",
        "        )\n",
        "\n",
        "        print(f\"\\nEpoch {epoch:03d}, Loss: {loss:.4f}\")\n",
        "        print_metrics(train_metrics, 'train')\n",
        "\n",
        "        # Save best model based on training accuracy\n",
        "        if best_train_metrics is None or train_metrics['accuracy'] > best_train_metrics['accuracy']:\n",
        "            best_train_metrics = train_metrics\n",
        "            best_epoch = epoch\n",
        "            # Save model state\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "\n",
        "            # Plot ROC curves for best epoch\n",
        "            plot_roc_curves(y_true_train, y_prob_train, num_classes, 'train')\n",
        "\n",
        "# Load the best model for final evaluation\n",
        "print(f\"\\nLoading best model from epoch {best_epoch}...\")\n",
        "model.load_state_dict(torch.load('best_model.pt'))\n",
        "\n",
        "######################################\n",
        "# Final Evaluation on Test Set\n",
        "######################################\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_metrics, y_true_test, y_pred_test, y_prob_test = compute_metrics_for_batch(\n",
        "    model, test_visit_data, test_patient_x, test_patient_edge_index,\n",
        "    test_visit_to_patient_mapping, test_visit_data.test_mask\n",
        ")\n",
        "\n",
        "# Print detailed metrics\n",
        "print_metrics(test_metrics, 'test')\n",
        "\n",
        "# Plot ROC curves for test set\n",
        "plot_roc_curves(y_true_test, y_prob_test, num_classes, 'test')\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"\\nTest Confusion Matrix:\")\n",
        "cm = confusion_matrix(y_true_test, y_pred_test)\n",
        "print(cm)\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nTest Classification Report:\")\n",
        "print(classification_report(y_true_test, y_pred_test, digits=4))\n",
        "\n",
        "# Create a confusion matrix plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(num_classes)\n",
        "plt.xticks(tick_marks, range(num_classes))\n",
        "plt.yticks(tick_marks, range(num_classes))\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "\n",
        "# Add text annotations to the confusion matrix\n",
        "thresh = cm.max() / 2\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix.png')\n",
        "plt.close()\n",
        "\n",
        "# Save final metrics to CSV\n",
        "train_metrics_df = pd.DataFrame({k: [v] for k, v in best_train_metrics.items()})\n",
        "test_metrics_df = pd.DataFrame({k: [v] for k, v in test_metrics.items()})\n",
        "\n",
        "metrics_df = pd.concat([\n",
        "    train_metrics_df.add_prefix('train_'),\n",
        "    test_metrics_df.add_prefix('test_')\n",
        "], axis=1)\n",
        "\n",
        "metrics_df.to_csv('model_metrics.csv', index=False)\n",
        "print(\"\\nMetrics saved to model_metrics.csv\")\n",
        "\n",
        "# Print summary of key metrics\n",
        "print(\"\\n===== SUMMARY =====\")\n",
        "print(f\"Best epoch: {best_epoch}\")\n",
        "print(f\"Train accuracy: {best_train_metrics['accuracy']:.4f}\")\n",
        "print(f\"Test accuracy: {test_metrics['accuracy']:.4f}\")\n",
        "\n",
        "if 'auc_macro' in best_train_metrics and 'auc_macro' in test_metrics:\n",
        "    print(f\"Train AUC (macro): {best_train_metrics['auc_macro']:.4f}\")\n",
        "    print(f\"Test AUC (macro): {test_metrics['auc_macro']:.4f}\")\n",
        "\n",
        "print(\"\\n===== GENERATING PREDICTION CSVs =====\")\n",
        "\n",
        "# Model parameters (using the same as defined in training)\n",
        "model_params = {\n",
        "    'visit_in_channels': test_visit_x.shape[1],\n",
        "    'patient_in_channels': test_patient_x.shape[1],\n",
        "    'hidden_channels': 128,\n",
        "    'num_classes': num_classes,\n",
        "    'dropout': 0.3,\n",
        "    'num_heads': 4\n",
        "}\n",
        "\n",
        "# Find model checkpoints\n",
        "model_paths = glob.glob('model_*.pt')\n",
        "if not model_paths:\n",
        "    model_paths = ['best_model.pt']  # Fallback to single best model\n",
        "\n",
        "print(f\"Using {len(model_paths)} models for ensemble predictions\")\n",
        "\n",
        "# Load ensemble models\n",
        "models = load_models(model_paths, MultiScaleGAT, model_params)\n",
        "\n",
        "# === GENERATE TEST PREDICTIONS ===\n",
        "# Prepare test data (convert diagnoses to 1-indexed for results)\n",
        "original_test_diagnoses = test_df['DIAGNOSIS'].copy()  # Save current diagnoses (0-indexed)\n",
        "test_df['DIAGNOSIS'] = original_test_diagnoses + 1  # Convert back to 1-indexed for results\n",
        "\n",
        "# Get ensemble predictions for test data\n",
        "test_ensemble_preds, test_ensemble_probs = get_ensemble_predictions(\n",
        "    models, test_visit_data, test_patient_x, test_patient_edge_index, test_visit_to_patient_mapping\n",
        ")\n",
        "\n",
        "# Create test prediction CSVs\n",
        "test_visit_pred_df = create_visit_level_csv(test_df, test_ensemble_preds, test_ensemble_probs, \"test_visit_predictions.csv\")\n",
        "test_patient_pred_df = create_patient_level_csv(test_df, test_ensemble_preds, test_ensemble_probs, \"test_patient_predictions.csv\")\n",
        "\n",
        "print(f\"Created test visit-level predictions: test_visit_predictions.csv\")\n",
        "print(f\"Created test patient-level predictions: test_patient_predictions.csv\")\n",
        "\n",
        "# === GENERATE TRAIN PREDICTIONS ===\n",
        "# Prepare train data (convert diagnoses to 1-indexed for results)\n",
        "original_train_diagnoses = train_df['DIAGNOSIS'].copy()  # Save current diagnoses (0-indexed)\n",
        "train_df['DIAGNOSIS'] = original_train_diagnoses + 1  # Convert back to 1-indexed for results\n",
        "\n",
        "# Get ensemble predictions for train data\n",
        "train_ensemble_preds, train_ensemble_probs = get_ensemble_predictions(\n",
        "    models, train_visit_data, train_patient_x, train_patient_edge_index, train_visit_to_patient_mapping\n",
        ")\n",
        "\n",
        "# Create train prediction CSVs\n",
        "train_visit_pred_df = create_visit_level_csv(train_df, train_ensemble_preds, train_ensemble_probs, \"train_visit_predictions.csv\")\n",
        "train_patient_pred_df = create_patient_level_csv(train_df, train_ensemble_preds, train_ensemble_probs, \"train_patient_predictions.csv\")\n",
        "\n",
        "print(f\"Created train visit-level predictions: train_visit_predictions.csv\")\n",
        "print(f\"Created train patient-level predictions: train_patient_predictions.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WS5IOkmfkbwY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}